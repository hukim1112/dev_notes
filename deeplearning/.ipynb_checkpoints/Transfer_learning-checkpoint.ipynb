{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Main slim library\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colab에서 실행하세요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hukim1112/DLCV_CLASS.git\n",
    "os.chdir('/content/DLCV_CLASS')\n",
    "from datasets import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tunning할 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "data_dir = '/content/sample_data'\n",
    "dataset_utils.download_and_uncompress_tarball(_DATA_URL, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained model 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHECKPOINT_URL = \"http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = '/content/sample_data'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(_CHECKPOINT_URL, checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## declare few paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/content/sample_data\"\n",
    "checkpoint_path = \"/content/sample_data\"\n",
    "train_dir = '../inception_finetuned/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input pipeline (이 부분은 넘기셔도 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_filenames_and_classes(dataset_dir):\n",
    "      flower_root = os.path.join(dataset_dir, 'flower_photos')\n",
    "      directories = []\n",
    "      class_names = []\n",
    "      for dir_name in os.listdir(flower_root):\n",
    "        path = os.path.join(flower_root, dir_name)\n",
    "        if os.path.isdir(path):\n",
    "          directories.append(path)\n",
    "          class_names.append(dir_name)\n",
    "\n",
    "      photo_filenames = []\n",
    "      for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "          path = os.path.join(directory, filename)\n",
    "          photo_filenames.append(path)\n",
    "\n",
    "      return photo_filenames, sorted(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths, class_names = _get_filenames_and_classes(dataset_path)\n",
    "class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "ids_to_class_name = dict(zip(range(len(class_names)), class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = filepaths[:3000]\n",
    "validate_data = filepaths[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    images_decoded = tf.image.decode_jpeg(image_string, channels = 3)\n",
    "    raw_images = tf.image.resize_images(images_decoded, [224, 224])\n",
    "    images = inception_preprocessing.preprocess_image(images_decoded, height = 224, width = 224, is_training=True)\n",
    "    \n",
    "    return images, raw_images, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without dataset pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/hukim1112/DLCV_CLASS/blob/master/datasets_without_pipelining.png?raw=true\" width=600></br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with dataset pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/hukim1112/DLCV_CLASS/blob/master/datasets_with_pipelining.png?raw=true\" width=600></br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "def load_batch(filepaths, num_images, batch_size=32):\n",
    "\n",
    "    dataset_filepath = tf.data.Dataset.from_tensor_slices(tf.cast(filepaths, tf.string))\n",
    "    dataset_class = tf.data.Dataset.from_tensor_slices(\n",
    "        [class_names_to_ids[os.path.basename(os.path.dirname(filepath))] for filepath in filepaths])\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((dataset_filepath, dataset_class))\n",
    "    dataset = dataset.shuffle(num_images)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(2)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, raw_images, labels = iterator.get_next()\n",
    "\n",
    "\n",
    "    return images, raw_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, raw_images, labels = load_batch(filepaths, num_images = len(filepaths), batch_size=32)\n",
    "# with tf.Session() as sess:\n",
    "#     for i in range(1):\n",
    "#         a = sess.run(images)\n",
    "\n",
    "#print(a.shape)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow는 graph 예시 (실행 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 X\n",
    "def my_cnn(images, num_classes, is_training):  # is_training is not used...\n",
    "    with slim.arg_scope([slim.max_pool2d], kernel_size=[3, 3], stride=2):\n",
    "        net = slim.conv2d(images, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.conv2d(net, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 192)\n",
    "        net = slim.fully_connected(net, num_classes, activation_fn=None)       \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dd0029d34fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_batch' is not defined"
     ]
    }
   ],
   "source": [
    "images, _, labels = load_batch(train_data, num_images = len(filepaths))\n",
    "\n",
    "scores = my_cnn(images, 5, is_training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 X\n",
    "class AE():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.batch_size = 128\n",
    "        with self.graph.as_default():\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.images, labels = get_data.data_pipeline('mnist', 128, 'train')\n",
    "            self.visualizer = Visualizer(exp_name='AE_mnist', row=8, col=8)\n",
    "            self.generated_images = self._build_graph(self.images, 5, reuse=tf.AUTO_REUSE, training=True)\n",
    "            self.loss = self._loss_function(self.images, self.generated_images)\n",
    "            self.solver = tf.train.AdamOptimizer(learning_rate=0.0001) \\\n",
    "                           .minimize(self.loss)\n",
    "            initializer = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(initializer)\n",
    "    def train(self, iteration):\n",
    "        for i in range(iteration+1):\n",
    "            loss, _, np_real_images, np_generated_images = self.sess.run(\n",
    "                    [self.loss, self.solver, self.images, self.generated_images])\n",
    "            if i % 10 == 0:\n",
    "                print(\"iterator {} : loss {} \".format(i, loss))\n",
    "            if i % 1000 == 0:\n",
    "                self.visualize(np_real_images, np_generated_images, i)\n",
    "            \n",
    "    def visualize(self, images, generated_images, i):\n",
    "        visual_imgs = np.concatenate( (images[:8*4], generated_images[:8*4]), axis = 0 )\n",
    "        visual_imgs = 255*(visual_imgs/2 + 0.5)\n",
    "        visual_imgs = visual_imgs.astype(int)\n",
    "        self.visualizer.draw_imgs(visual_imgs)\n",
    "        self.visualizer.save_fig(name=\"AE_mnist_iter_{}\".format(str(i)))\n",
    "        \n",
    "    def _build_graph(self, input, dim_code, reuse=tf.AUTO_REUSE, training=False):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "        net = layers.conv2d(input, 64, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 32, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 32, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 128, weights_regularizer=regularizer)\n",
    "        latent_var = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "        \n",
    "        net = layers.fully_connected(latent_var, 128, weights_regularizer=regularizer)\n",
    "        net = layers.fully_connected(latent_var, 7*7*32, weights_regularizer=regularizer)\n",
    "        net = tf.reshape(shape=[-1, 7, 7, 32], tensor=net)\n",
    "        net = layers.conv2d_transpose(net, 32, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 32, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 1, (3,3), 1, activation_fn=tf.nn.tanh, weights_regularizer=regularizer)\n",
    "        return net\n",
    "    def _loss_function(self, _real_images, _generated_images):\n",
    "        recon_loss = tf.reduce_mean(tf.square(_real_images - _generated_images))\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model on a different set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this may take several minutes.\n",
    "\n",
    "import os\n",
    "\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "\n",
    "\n",
    "def get_init_fn():\n",
    "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "    checkpoint_exclude_scopes=[\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "    \n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "\n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "      os.path.join(checkpoint_path, 'inception_v1.ckpt'),\n",
    "      variables_to_restore)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    images, _, labels = load_batch(train_data, num_images = len(filepaths))\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes= len(class_names), is_training=True)\n",
    "        \n",
    "    # Specify the loss function:\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, num_classes = len(class_names))\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        init_fn=get_init_fn(),\n",
    "        number_of_steps=100)\n",
    "        \n",
    "  \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "batch_size = 10\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "\n",
    "    images, images_raw, labels = load_batch(validate_data, num_images = len(filepaths))\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes= len(class_names), is_training=True)\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "            np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "    \n",
    "            for i in range(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                true_label = np_labels[i]\n",
    "                predicted_label = np.argmax(np_probabilities[i, :])\n",
    "                predicted_name = ids_to_class_name[predicted_label]\n",
    "                true_name = ids_to_class_name[true_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "    # Define the metrics:\n",
    "    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "        'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\n",
    "        'eval/Recall@5': slim.metrics.streaming_recall_at_k(logits, labels, 5),\n",
    "    })\n",
    "\n",
    "    print('Running evaluation Loop...')\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    metric_values = slim.evaluation.evaluate_once(\n",
    "        master='',\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        logdir=train_dir,\n",
    "        eval_op=list(names_to_updates.values()),\n",
    "        final_op=list(names_to_values.values()))    \n",
    "\n",
    "    names_to_values = dict(zip(names_to_values.keys(), metric_values))\n",
    "    for name in names_to_values:\n",
    "        print('%s: %f' % (name, names_to_values[name]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
